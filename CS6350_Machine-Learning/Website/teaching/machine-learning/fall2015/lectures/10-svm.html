<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Support Vector Machines | Machine Learning |
    Fall 2015</title>
    <meta name="description" content="Machine learning (CS 5350/CS 6350), Fall 2015
">

    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/extra.css">    
    <link rel="canonical" href="10-svm.html">
</head>

  
  <body>
    <div id="content">
      <header class="top">
  <h1 class="title">
    <a href="../index.html" style="text-decoration:none">Machine Learning</a>
  </h1>
  <div id="title-email"">CS 5350/6350, Fall 2015</div>  
</header>


        <hr/>
<center>
  <div class="nav">
    <ul>
      <li><a href="../index.html">home</a></li>
      <li><a href="../info.html">information</a></li>
      <li><a href="../topics.html">topics</a></li>
      <li><a href="../lectures.html">lectures</a></li>
      <li><a href="../assignments.html">homeworks</a></li>
      <li><a href="../projects.html">projects</a></li>
      <li><a href="../resources.html">resources</a></li>
    </ul>
  </div>
</center>
        
        <div class="main">
           <h2> Support Vector Machines</h2> 
          <div class="post" style="margin-top:1em;">

  <center style="font-size:80%; margin-top:1em">
    
    <a href="09-boosting-ensembles.html">&larr;Previous lecture</a>
     &nbsp;|&nbsp;
    <a href="../index.html">Home</a> &nbsp;|&nbsp;
    
    <a href="11-risk-minimization.html">Next lecture &rarr;</a>    
        
  </center>
  
  
  <p>In this lecture, we will look at support vector machines. We will
first look at the connection between maximizing margins and learning
linear classifiers. This will give us an objective function for
training, namely the SVM objective. This objective is our first sight
of the idea of regularized risk minimization.</p>

<p>There are several algorithms for optimizing the SVM objective. We will
look at a simple, yet effective, one: stochastic sub-gradient descent
and explore the connection with the perceptron algorithm. We will end
this lecture with a look at the dual formulation of the SVM objective
and kernels.</p>

<h3 id="links">Links</h3>

<ul>
  <li>
    <p><a href="../slides/10-svm/svm.pdf">Lecture slides</a></p>
  </li>
  <li>
    <p>Chapters 3 and 6 of Hal Daum√© III,
<a href="http://ciml.info/">A Course in Machine Learning</a></p>
  </li>
</ul>

<h3 id="additional-reading">Additional reading</h3>
<ul>
  <li>
    <p><a href="https://web.stanford.edu/class/ee364b/lectures/stoch_subgrad_slides.pdf">Notes on the stochastic subgradient method</a></p>
  </li>
  <li>
    <p><a href="https://web.stanford.edu/class/ee364a/lectures/functions.pdf">Convex functions</a></p>
  </li>
  <li>
    <p><a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">Stochastic gradient descent tricks</a></p>
  </li>
</ul>


  <center style="font-size:80%; margin-top:1em">
    
    <a href="09-boosting-ensembles.html">&larr;Previous lecture</a>
     &nbsp;|&nbsp;
    <a href="../index.html">Home</a> &nbsp;|&nbsp;
    
    <a href="11-risk-minimization.html">Next lecture &rarr;</a>    
        
  </center>
</div>

        </div>
        <hr/>
<center>
  <div class="nav">
    <ul>
      <li><a href="../index.html">home</a></li>
      <li><a href="../info.html">information</a></li>
      <li><a href="../topics.html">topics</a></li>
      <li><a href="../lectures.html">lectures</a></li>
      <li><a href="../assignments.html">homeworks</a></li>
      <li><a href="../projects.html">projects</a></li>
      <li><a href="../resources.html">resources</a></li>
    </ul>
  </div>
</center>


    </div>
  </body>
</html>
