<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Course topics | Machine Learning |
    Fall 2015</title>
    <meta name="description" content="Machine learning (CS 5350/CS 6350), Fall 2015
">

    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/extra.css">    
    <link rel="canonical" href="topics.html">
</head>

  
  <body>
    <div id="content">
      <header class="top">
  <h1 class="title">
    <a href="index.html" style="text-decoration:none">Machine Learning</a>
  </h1>
  <div id="title-email"">CS 5350/6350, Fall 2015</div>  
</header>


        <hr/>
<center>
  <div class="nav">
    <ul>
      <li><a href="index.html">home</a></li>
      <li><a href="info.html">information</a></li>
      <li><a href="topics.html">topics</a></li>
      <li><a href="lectures.html">lectures</a></li>
      <li><a href="assignments.html">homeworks</a></li>
      <li><a href="projects.html">projects</a></li>
      <li><a href="resources.html">resources</a></li>
    </ul>
  </div>
</center>
        
        <div class="main">
           <h2> Course topics</h2> 
          <article class="post-content">
  <p>The following is a tentative list (a superset, actually) of the topics
to be covered in the class. Note: This list will change as we get
closer to the start of the semester and also as the semester
progresses. For a listing of lectures and schedules, visit the
<a href="http://svivek.com/lectures.html">lectures page</a>.</p>

<ol>
  <li>Introduction to machine learning
    <ul>
      <li>Examples</li>
      <li>What: learning functions from data</li>
      <li>Scope: Mostly binary classification</li>
      <li>Main questions covered:
        <ul>
          <li>Modeling: Thinking of your problem as a machine learning problem</li>
          <li>Supervision: Fully supervised, unsupervised, and in between</li>
          <li>Algorithms: What does it mean to learn?</li>
          <li>Representing data (and finding good representations)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Supervised learning setup
    <ul>
      <li>Formal definition via instances, concept spaces, hypothesis space, and learning algorithms</li>
    </ul>
  </li>
  <li>Learning decision trees
    <ul>
      <li>Algorithms for learning decision trees</li>
      <li>Decision boundaries of classifiers, what Boolean functions can trees represent?</li>
    </ul>
  </li>
  <li>Linear classifiers
    <ul>
      <li>What Boolean functions can linear classifiers represent?</li>
      <li>Linear regression</li>
      <li>Online learning</li>
      <li>Additive and multiplicative update algorithms (Perceptron, Winnow and variants)</li>
      <li>Mistake bound</li>
      <li>The kernel trick</li>
    </ul>
  </li>
  <li>Computational Learning theory
    <ul>
      <li>Quantifying performance and generalization (a.k.a how can we trust the classifiers?)</li>
      <li>PAC learnability, Chernoff/Hoeffding bounds</li>
      <li>Overfitting vs generalization, bias-variance tradeoff</li>
      <li>Infinite hypothesis spaces, shattering, VC dimension</li>
      <li>Algorithmic questions:
        <ul>
          <li>From mistake bound to generalization → Voted/averaged perceptron</li>
          <li>Data dependent VC dimension → SVM</li>
          <li>Weak learnability → Boosting</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Boosting
    <ul>
      <li>Does weak learnability imply strong learnability?</li>
      <li>AdaBoost algorithm and theory</li>
      <li>Ensemble methods: Bagging, random forests, boosted decision trees</li>
    </ul>
  </li>
  <li>Support vector machine
    <ul>
      <li>Maximizing margin and SVM objective</li>
      <li>Kernels again</li>
      <li>Optimization, SGD and comparison to perceptron</li>
      <li>Empirical risk minimization, regularization</li>
    </ul>
  </li>
  <li>Probabilistic learning
    <ul>
      <li>Bayesian learning: Independence assumptions, Maximum Likelihood Estimation</li>
      <li>Naive Bayes
        <ul>
          <li>Learning</li>
          <li>Naive bayes as a linear classifier and expressivity</li>
        </ul>
      </li>
      <li>Logistic regression
        <ul>
          <li>Formulation, LR objective, minimization</li>
          <li>Empirical risk minimization and comparison to SVM</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Learning with missing labels
    <ul>
      <li>EM algorithm</li>
      <li>Semi-supervised learning, bootstrapping, active learning</li>
      <li>Unsupervised learning</li>
      <li>Clustering, K-Means</li>
      <li>Dimensionality reduction, PCA, SVD</li>
      <li>Learning representations (possibly with pointers to deep learning)</li>
    </ul>
  </li>
  <li>Multiclass classification
    <ul>
      <li>One-vs-all, all-vs-all</li>
      <li>Multiclass extensions of SVM, logistic regression and Perceptron</li>
    </ul>
  </li>
  <li>Practical concerns
    <ul>
      <li>Features and feature selection</li>
      <li>Experimental methodology: model selection, cross-validation</li>
      <li>Software and practical advice on how to use the learning algorithms</li>
    </ul>
  </li>
</ol>

</article>

        </div>
        <hr/>
<center>
  <div class="nav">
    <ul>
      <li><a href="index.html">home</a></li>
      <li><a href="info.html">information</a></li>
      <li><a href="topics.html">topics</a></li>
      <li><a href="lectures.html">lectures</a></li>
      <li><a href="assignments.html">homeworks</a></li>
      <li><a href="projects.html">projects</a></li>
      <li><a href="resources.html">resources</a></li>
    </ul>
  </div>
</center>


    </div>
  </body>
</html>
