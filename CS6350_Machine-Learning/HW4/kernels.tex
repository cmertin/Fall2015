\section{Kernels and the Perceptron algorithm}
\label{sec:kernel}

In the class, we saw how the idea of kernels can be applied to
increase the expressiveness of the SVM classifier. Kernels is more
broadly applicable than just SVMs. In this question, we will see how
kernels can be used with the Perceptron algorithm.

Suppose, we wish to learn a $k$-DNF (not necessarily monotone) using a
kernel version of Perceptron. Recall that a $k$-DNF is a Boolean
function that is a disjunction of conjunctive clauses, where each
conjunctive clause has exactly $k$ literals. For example, the
following Boolean functions are $2$-DNFs:
\begin{itemize}
\item $(x_1 \wedge x_2) \vee (\neg x_2 \wedge x_3)$
\item $(\neg x_1 \wedge x_2) \vee (x_3 \wedge x_4) \vee (\neg x_4 \wedge x_1)$
\end{itemize}

In order to learn a $k$-DNF, we will define a feature transformation
$\phi(\bx)$ that maps examples $\bx \in \{0,1\}^n$ to a new space of
$k$-conjunctions and then define a kernel Perceptron algorithm for
learning. That is, each element of $\phi(\bx)$ corresponds to the
value of a different $k$-conjunction and $\phi$ enumerates over all
$k$-conjunctions. After this transformation, the classification of the
Perceptron is via the sign of the dot product of the weight vector and
the example $\bw^T\phi(x)$. The goal is to represent this dot product
using a kernel $K(w, x)$.


\begin{enumerate}
\item ~[5 points] Show that any $k$-DNF is linearly separable after
  the feature transformation.

\begin{itemize}
\item As was proven in the last homework, $k$-Decision Lists ($k$-DL) are linearly separable. Therefore, this can be proven by showing that $k$-DNF functions are similar to that of $k$-DL's.

Assume we can have an arbitrary $k$-DNF of the form $f = \vee_{i=1}^{N}c_{i}$, where $c_{i}$ is a $k$-conjunction. We can turn this into a $k$-DL by the following form $C = \left\{(c_{1},1),(c_{2},1),\ldots,(c_{N},1),0  \right\}$. From this definition, it is easy to see that {\em both} $C$ and $f$ are satisfied as long as one of the $k$-conjunctions are true. Therefore, the $k$-DL and $k$-DNF behave in the same way, so the new $k$-DL can be used to make it linearly separable.
\end{itemize} 

\item ~[10 points] Let $C$ be the set of all conjunctions containing
  exactly $k$ different literals. (Recall that a literal is a Boolean
  variable or its negation.) For any $\bx_1, \bx_2 \in \{0, 1\}^n$,
  define
  $$K(\bx_1, \bx_2) = \phi(\bx_1)^T\phi(\bx_2) = \sum_{c \in C} c(\bx_1)c(\bx_2).$$ 

  Here $c(\bx)$ is the value of a conjunction $c$ using the values in
  $\bx$. Show that this function can be computed efficiently without
  explicitly computing the values of $\phi(\bx_1)$ and $\phi(\bx_2)$.

\begin{itemize}
\item The function can be efficiently computed in {\em linear time} as follows.

There are $\mathcal{O}(N)$ $k$-conjunctions $\in\ C$, which would be the upper bound on simply calcualting each of the conjunctions istead of worrying about mapping to the new space and taking the dot product. Since they are conjunctions, $c\left(\mathbf{x}_{1}\right)c\left(\mathbf{x}_{2}\right)=1$ when $c\left(\mathbf{x}_{1}\right)$ {\em and} $c\left(\mathbf{x}_{2}\right)$ are {\em both} true. Therefore, instead of mapping all of the data to the new feature space and taking the dot product, {\em at most} linear time can be achieved by simply looking at the ones with the same number of active features between the two sets and summing over those which agree.
\end{itemize} 

\item ~[5 points] Assume that the initial weight vector for Perceptron
  is the zero vector. Then, show that
  $\bw = \sum_{(\bx_i, y_i) \in M} y_i \, \phi(x_i)$. Here
  $M = \{(\bx_i, y_i)\}$ is the set of examples on which the learning
  algorithm made mistakes.

\begin{itemize}
\item The Perceptron algorithm only updates everytime it makes a mistake. To update the weight vector, it updates it as $\mathbf{w}_{i+1} = \mathbf{w}_{i} + y_{i}\mathbf{x}_{i}$. If we have a function $\phi(\mathbf{x})$ that maps $\mathbf{x}$ to a new feature space, we have $\mathbf{w}_{i+1} = \mathbf{w}_{i} + y_{i}\phi(\mathbf{x}_{i})$. If the Perceptron algorithm makes $M$ errors, the resulting weight vector can be reprsented as $\mathbf{w} = \sum_{(\mathbf{x}_{i},y_{i})\in M} y_{i}\phi(\mathbf{x}_{i})$.
\end{itemize} 

\item ~[5 points] Using the fact that the weight vector can be written
  as in the previous question, write the classification step
  $y=sgn\left(\bw^T\phi(\bx)\right)$ using the kernel function
  $\kappa(\bx_1,\bx_2)$ instead of using the explicit feature
  representations.

\begin{itemize}
\item From the question we have

\begin{align*}
y &= sgn\left( \mathbf{w}^{T}\phi\left(\mathbf{x} \right)\right)\\
\intertext{Which can be generalized by plugging in for $\mathbf{w}$ from the question before as}
y &= sgn\left( \sum_{(\bx_i, y_i) \in M} y_i \, \phi(\mathbf{x}_i)^{T}\phi(\mathbf{x})\right)\\
\intertext{We also know from the notes that $\kappa(\mathbf{x},\mathbf{z}) \equiv \phi(\mathbf{x})^{T}\phi(\mathbf{z})$. So we can use this to rewrite it in the form}
y &= sgn\left(\sum_{(\bx_i, y_i) \in M} y_i \, \kappa(\mathbf{x}_{i},\mathbf{x}) \right)
\end{align*}
\end{itemize} 

\item ~[15 points] Write explicitly the pseudo code for the kernel
  Perceptron algorithm that uses your kernel to learn a $k$-DNF.
  (Hint: Instead of storing the weight vector, you will store a list
  of examples where the algorithm makes mistakes.)

\begin{itemize}
\item The given problem states that the initial weight vector is the zero vector, therefore there is no bias term to account for in the update, so it will be ignored.

\begin{algorithm}
\caption{Zero'd Kernel Perceptron}
\begin{algorithmic}
\STATE $\mathbf{w}\leftarrow 0$
\FOR{$i=1$ \TO $M$}
    \IF{$y\neq sgn\left( \sum_{\bx_{i},y_{i}\in M} y_{i}\kappa(\mathbf{x},\mathbf{x}_{i})\right)$}
       \STATE{$\mathbf{w}\leftarrow \mathbf{w}\cup(\mathbf{x}_{i},y)$}
    \ENDIF
\ENDFOR
\RETURN $sgn\left( \sum_{\bx_{i},y_{i}\in M} y_{i}\kappa(\mathbf{x},\mathbf{x}_{i})\right)$
\end{algorithmic}
\end{algorithm}
\end{itemize} 
\end{enumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw4"
%%% End:
